{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EqmqlpdxyS7t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), \"pytorch_geometric_temporal\"))\n",
        "# https://stackoverflow.com/questions/35569042/ssl-certificate-verify-failed-with-python3\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh2t-N5D99uq",
        "outputId": "3a0fecca-74ee-426c-ca07-a356d721e413"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKghKWgqyS7u",
        "outputId": "f0013707-3b95-48ce-9062-c367b132b3d4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt24cu121)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt24cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "!pip install torch==2.4.0\n",
        "\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LKAqaGbuyS7v"
      },
      "outputs": [],
      "source": [
        "import torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gPIryJDiyS7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790aced6-e435-4271-b3e9-6cc810e734ce",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/\n",
        "\n",
        "from pytorch_geometric_temporal.torch_geometric_temporal.dataset import PemsBayDatasetLoader, METRLADatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pFjTgmVjyS7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ef1d44-fe8a-4e06-8294-a393ecafc512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "# Load in the data\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "#from pytorch_geometric_temporal.torch_geometric_temporal.dataset import PemsBayDatasetLoader\n",
        "%cd /content/drive/MyDrive/\n",
        "from src.utils.data_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Scaler:\n",
        "  def __init__(self, shift: np.ndarray, scale: np.ndarray):\n",
        "      \"\"\"\n",
        "      shift: [F,] -> [1, F, 1]\n",
        "      scale: [F,] -> [1, F, 1]\n",
        "      \"\"\"\n",
        "      self.shift = shift.reshape(1, -1, 1)\n",
        "      self.scale = scale.reshape(1, -1, 1)\n",
        "      print(self.shift.shape)\n",
        "      print(self.scale.shape)\n",
        "\n",
        "  def normalize(self, data: torch.Tensor) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      data: [V, F, W + H]\n",
        "      ---\n",
        "      data_norm: [V, F, W + H]. data_norm = (data - shift) / scale\n",
        "      \"\"\"\n",
        "      return (data - self.shift) / self.scale\n",
        "\n",
        "  def unnormalize(self, data: torch.Tensor) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      data: [V, F, W + H]\n",
        "      ---\n",
        "      data_unnorm: [V, F, W + H]. data_unnorm = shift + scale * data\n",
        "      \"\"\"\n",
        "      return self.shift + self.scale * data\n",
        "\n",
        "  def unnormalize_y(self, data: torch.Tensor) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Unnormalizes data of shape [batch_size, num_nodes] to match the scaler's shift and scale.\n",
        "      \"\"\"\n",
        "\n",
        "      # Add the feature dimension to match the scaler (reshape to [batch_size, 1, num_nodes])\n",
        "      data = data.unsqueeze(1)  # Shape becomes [batch_size, 1, num_nodes]\n",
        "\n",
        "      # Expand shift and scale to match data shape [batch_size, 1, num_nodes]\n",
        "      # Convert shift and scale to torch tensors if they are numpy arrays\n",
        "      shift = torch.tensor(self.shift, device=data.device)\n",
        "      scale = torch.tensor(self.scale, device=data.device)\n",
        "      shift = shift.expand_as(data)\n",
        "      scale = scale.expand_as(data)\n",
        "\n",
        "      # Perform unnormalization\n",
        "      unnormalized_data = shift + scale * data\n",
        "      #print(f\"Unnormalized data shape: {unnormalized_data.shape}\")\n",
        "      return unnormalized_data.squeeze(1)  # Remove the feature dimension\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TimeSeriesDataset:\n",
        "    # TODO: This only works on PemsBayDatasetLoader & METRLADatasetLoader since their dataloaders are\n",
        "    # modified to have indices and not be normalized using the entire dataset (preventing data leakage).\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataloader,\n",
        "        window: int = 12,\n",
        "        horizon: int = 12,\n",
        "        train: float = 0.7,\n",
        "        test: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        dataloader: One of the data loader objects defined under torch_geometric_temporal/dataset\n",
        "        window (W): Number of timesteps the model is allowed to look-back\n",
        "        horizon (H): Number of timesteps to predict ahead\n",
        "        train: Ratio of data assigned to training. See split_data()\n",
        "        test: Ratio of data assigned to testing. See split_data()\n",
        "        ---\n",
        "        indices: List of (sample_start, sample_end) tuples that specifies the start (t - W + 1)\n",
        "            & end (t + H) indices of each data slice (t - W + 1, ..., t, t + 1, ..., t + H)\n",
        "        X: np.array[V, F, T], V=num_nodes, F=num_features, T=num_timesteps. Raw dataset\n",
        "        data: Graph signal object defined under torch_geometric_temporal/signal\n",
        "        \"\"\"\n",
        "        self.data = dataloader.get_dataset(num_timesteps_in=window, num_timesteps_out=horizon)\n",
        "        self.indices = dataloader.indices\n",
        "        self.X = dataloader.X.detach().cpu().numpy().astype(np.float32)\n",
        "        #print(self.X.shape, \"hello\")\n",
        "        if len(self.X.shape) == 2: # [V, T]\n",
        "            self.X = self.X.reshape(self.X.shape[0], 1, self.X.shape[1]) # [V, F=1, T]\n",
        "        assert len(self.X.shape) == 3, \"Missing dimension(s) in the raw dataset\"\n",
        "        self.split_data(train=train, test=test)\n",
        "\n",
        "    def split_data(self, train: float = 0.7, test: float = 0.2):\n",
        "        \"\"\"\n",
        "        train: Ratio of data assigned to training. num_train = round(train * len_of_data)\n",
        "        test: Ratio of data assigned to testing. num_train_test = num_train + round(test * len_of_data)\n",
        "        ---\n",
        "        data_splits: Dict of graph signal objects defined under torch_geometric_temporal/signal\n",
        "            - train: data[:num_train]\n",
        "            - test: data[num_train:num_train_test]\n",
        "            - valid: data[num_train_test:]\n",
        "        scaler: Scaler object with shift & scale set to the mean & std of the training samples\n",
        "            (across all nodes and training timesteps)\n",
        "        \"\"\"\n",
        "        assert train > 0, \"Train ratio must be positive\"\n",
        "        num_train = round(train * self.data.snapshot_count)\n",
        "        num_train_test = num_train + round(test * self.data.snapshot_count)\n",
        "        # Create data splits\n",
        "        self.data_splits = {\n",
        "            \"train\": self.data[:num_train],\n",
        "            \"test\": (\n",
        "                self.data[num_train:num_train_test]\n",
        "                if num_train_test <= self.data.snapshot_count else None\n",
        "            ),\n",
        "            \"valid\": (\n",
        "                self.data[num_train_test:]\n",
        "                if num_train_test < self.data.snapshot_count else None\n",
        "            )\n",
        "        }\n",
        "        # Create Scaler based on the mean & std of training data\n",
        "        num_train_samp = self.indices[num_train - 1][-1] # Index of last sample in train split\n",
        "        X_train = self.X[:, 0:1, :num_train_samp] # TAKE ONLY THe FIRST FEATURE\n",
        "        #print(X_train.shape, \"x train shape\")\n",
        "        self.scaler = Scaler(\n",
        "            shift=np.mean(X_train, axis=(0, 2)),\n",
        "            scale=np.std(X_train, axis=(0, 2))\n",
        "        )"
      ],
      "metadata": {
        "id": "1V2re2CEjAUO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def masked_mae_tf(preds, labels, null_val=np.nan):\n",
        "    \"\"\"\n",
        "    Accuracy with masking.\n",
        "    :param preds:\n",
        "    :param labels:\n",
        "    :param null_val:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~tf.math.is_nan(labels)\n",
        "    else:\n",
        "        mask = tf.not_equal(labels, null_val)\n",
        "\n",
        "    # Ensure mask has the same shape as labels\n",
        "    mask = tf.expand_dims(tf.cast(mask, tf.float32), axis=-1)\n",
        "    mask_shape = tf.shape(mask)\n",
        "    labels_shape = tf.shape(labels)\n",
        "\n",
        "    #print(\"labels shape\", labels_shape)\n",
        "    mask = tf.squeeze(mask, axis=-1)\n",
        "    #print(\"mask shape\", mask.shape)\n",
        "\n",
        "    # Replace NaN values in labels with the mean of non-NaN values\n",
        "    labels = tf.where(tf.math.is_nan(labels), tf.reduce_mean(labels * mask, axis=-1, keepdims=True), labels)\n",
        "\n",
        "    loss = tf.abs(tf.subtract(preds, labels))\n",
        "    loss = loss * mask\n",
        "    loss = tf.where(tf.math.is_nan(loss), tf.zeros_like(loss), loss)\n",
        "\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return torch.tensor(loss.numpy(), device='cuda:0', requires_grad=True)"
      ],
      "metadata": {
        "id": "Pqfz8o1AwHTF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "z16730WhyS7v",
        "outputId": "5f710291-bf41-4c0e-bb04-260baaac95b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-a20d9cf77676>:73: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 1)\n",
            "(1, 1, 1)\n",
            "Scaler is enabled: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8107, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2473, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5270, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9927, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8592, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8619, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.1786, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8228, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1788, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.5137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7366, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3675, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2701, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.5410, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6057, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.4246, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.1327, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0736, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7967, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7440, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7903, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4488, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7384, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1138, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7982, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4413, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.4183, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4644, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7470, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7482, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1159, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1688, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.9623, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3266, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6239, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8693, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5190, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3638, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7080, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7538, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.5767, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9835, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0493, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.3716, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.4699, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.6923, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2920, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3763, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.5929, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9730, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.3855, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0938, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7673, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3441, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2772, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.9757, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9945, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3579, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7680, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.9453, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.0574, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.0104, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3192, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0409, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7479, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7067, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3766, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6130, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6053, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8527, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.6259, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1873, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1856, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.5330, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1496, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1710, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5397, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9480, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1764, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.5351, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7340, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8306, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8088, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7215, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7142, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2024, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1936, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.6803, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6457, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0620, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.1515, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9025, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1995, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.8822, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9617, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8347, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7471, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0972, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0567, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9092, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7216, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7727, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2507, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2333, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5823, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.5595, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.9125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9405, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5704, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9770, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8833, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4651, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6888, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8242, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1731, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.6872, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1986, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2097, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3939, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.4050, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.1635, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0815, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2108, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9056, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1360, device='cuda:0', grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "\r  1%|          | 1/100 [00:45<1:14:42, 45.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "Epoch 0 - Gradient norm before unscaling: nan\n",
            "Epoch 0 - Gradient norm after unscaling: nan\n",
            "Epoch 0, LR: 0.010000, Loss: 9.0178, RMSE@15min: 5.1653, RMSE@30min: 5.4994, RMSE@60min: 5.3649\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8107, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2473, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5270, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9927, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8592, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8619, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.1786, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8228, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1788, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.5137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7366, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3675, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2701, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.5410, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6057, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.4246, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.1327, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0736, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7967, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7440, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7903, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4488, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7384, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1138, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7982, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4413, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.4183, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4644, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7470, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7482, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1159, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1688, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.9623, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3266, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6239, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8693, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5190, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3638, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7080, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7538, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.5767, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9835, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0493, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.3716, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.4699, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.6923, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2920, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3763, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.5929, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9730, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.3855, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0938, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7673, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3441, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2772, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.9757, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9945, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3579, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7680, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.9453, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.0574, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.0104, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3192, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0409, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7479, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7067, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3766, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6130, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6053, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8527, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.6259, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1873, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1856, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.5330, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1496, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1710, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5397, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9480, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1764, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.5351, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7340, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8306, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8088, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7215, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7142, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2024, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1936, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.6803, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6457, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0620, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.1515, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9025, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1995, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.8822, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9617, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8347, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7471, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0972, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0567, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9092, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7216, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7727, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2507, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2333, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5823, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.5595, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.9125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9405, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5704, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9770, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8833, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4651, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6888, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8242, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1731, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.6872, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1986, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2097, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3939, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.4050, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.1635, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0815, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2108, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9056, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1360, device='cuda:0', grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|         | 2/100 [01:29<1:13:02, 44.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "Epoch 1 - Gradient norm before unscaling: nan\n",
            "Epoch 1 - Gradient norm after unscaling: nan\n",
            "Epoch 1, LR: 0.010000, Loss: 9.0178, RMSE@15min: 5.1653, RMSE@30min: 5.4994, RMSE@60min: 5.3649\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8107, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2473, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5270, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9927, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8592, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8619, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.1786, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8228, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1788, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.5137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7366, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3675, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2701, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.5410, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6057, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.4246, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.1327, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0736, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7967, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7440, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7903, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4488, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7384, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1138, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7982, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4413, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.4183, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4644, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7470, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7482, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1159, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1688, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.9623, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3266, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6239, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8693, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5190, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3638, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7080, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7538, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.5767, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9835, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0493, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.3716, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.4699, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.6923, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2920, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3763, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.5929, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9730, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.3855, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0938, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7673, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3441, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2772, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.9757, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9945, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3579, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7680, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.9453, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.0574, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.0104, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.3192, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0409, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7479, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7067, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3766, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6130, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6053, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8527, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.6259, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1873, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1856, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.5330, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1496, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.1710, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5397, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9480, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1764, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(6.5351, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7340, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8306, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8088, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.7215, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7142, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2024, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1936, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.6803, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.6748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6457, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9820, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0620, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.1515, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9025, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1995, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.1870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.8822, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.9617, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8347, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7471, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.0972, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.0567, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9092, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8563, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7216, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7727, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2507, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2333, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5823, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.5595, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.9125, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9405, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.5704, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9770, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8833, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4651, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.6888, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.5998, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8242, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1731, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.6872, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1986, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2097, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3939, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.4050, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.1635, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0815, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2108, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9056, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1360, device='cuda:0', grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|         | 3/100 [02:13<1:11:54, 44.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1653, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "Epoch 2 - Gradient norm before unscaling: inf\n",
            "Epoch 2 - Gradient norm after unscaling: nan\n",
            "Epoch 2, LR: 0.010000, Loss: 9.0178, RMSE@15min: 5.1653, RMSE@30min: 5.4994, RMSE@60min: 5.3649\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.2884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8107, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.2473, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5270, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9927, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.8592, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.8619, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.1786, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8228, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.1788, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(13.5137, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7366, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3675, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7870, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2701, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.5410, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6057, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.1748, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.4246, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(14.1327, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.0736, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7967, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7440, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7903, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4488, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7384, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1138, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.7982, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.4413, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.4183, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.4644, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.7470, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.7482, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1159, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.1688, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.9623, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.3266, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.6239, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.8693, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(7.5190, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.3638, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.7080, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.7538, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.5767, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(9.9835, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(5.0493, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.7884, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(8.3716, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.4699, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(12.6923, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.2920, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(10.3763, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(11.8795, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.5929, device='cuda:0', grad_fn=<SqrtBackward0>)\n",
            "y_hat device: cuda:0\n",
            "batch_y device: cuda:0\n",
            "y_hat dtype: torch.float32\n",
            "batch_y dtype: torch.float32\n",
            "Masked mse:  tensor(4.9730, device='cuda:0', grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|         | 3/100 [02:32<1:21:57, 50.70s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-a20d9cf77676>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mbatch_y_60\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msnapshot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_snapshots\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 60min (12th step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_edge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;31m#print(\"y hat shape\", y_hat.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0my_hat_15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 15min prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-a20d9cf77676>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# First DCRNN layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#print(f\"Timestep {t} - Before DCRNN1 - input shape: {current_x.shape}, h1 shape: {None if h1 is None else h1.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcrnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m#print(f\"Timestep {t} - After DCRNN1 - h1 shape: {h1.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/pytorch_geometric_temporal/torch_geometric_temporal/nn/recurrent/dcrnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, edge_index, edge_weight, H)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_update_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_reset_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mH_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_candidate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_tilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/pytorch_geometric_temporal/torch_geometric_temporal/nn/recurrent/dcrnn.py\u001b[0m in \u001b[0;36m_calculate_candidate_state\u001b[0;34m(self, X, edge_index, edge_weight, H, R)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_candidate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mH_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mH_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_x_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_tilde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mH_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_tilde\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mH_tilde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/pytorch_geometric_temporal/torch_geometric_temporal/nn/recurrent/dcrnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPyTorch\u001b[0m \u001b[0mFloat\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mHidden\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mall\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0madj_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dense_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0madj_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         deg_out = torch.matmul(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_to_dense_adj.py\u001b[0m in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_num_nodes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_num_nodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_num_nodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflattened_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.*is in beta and the API may change.*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     def scatter(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pytorch_geometric_temporal.torch_geometric_temporal.nn.recurrent import DCRNN\n",
        "from pytorch_geometric_temporal.torch_geometric_temporal.signal import temporal_signal_split\n",
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    def tqdm(iterable):\n",
        "        return iterable\n",
        "\n",
        "# Load and examine data\n",
        "loader = PemsBayDatasetLoader()\n",
        "df = TimeSeriesDataset(PemsBayDatasetLoader())\n",
        "train_dataset, test_dataset = df.data_splits[\"train\"], df.data_splits[\"test\"]\n",
        "\n",
        "class RecurrentGCN(torch.nn.Module):\n",
        "    def __init__(self, node_features):\n",
        "        super(RecurrentGCN, self).__init__()\n",
        "        self.dcrnn1 = DCRNN(in_channels=node_features, out_channels=64, K=2)\n",
        "        self.dcrnn2 = DCRNN(in_channels=64, out_channels=64, K=2)\n",
        "        self.linear = torch.nn.Linear(64, 12)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        \"\"\"\n",
        "        x: [batch_size, num_nodes, features, timesteps]\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, features, timesteps = x.size()\n",
        "        #print(f\"Input shapes: batch_size={batch_size}, num_nodes={num_nodes}, features={features}, timesteps={timesteps}\")\n",
        "\n",
        "        # Initialize hidden states\n",
        "        h1 = None\n",
        "        h2 = None\n",
        "\n",
        "        # Process each timestep while maintaining batch efficiency\n",
        "        for t in range(timesteps):\n",
        "            # Get current timestep data [batch_size, num_nodes, features]\n",
        "            current_x = x[:, :, :, t]\n",
        "            #print(f\"Timestep {t} - current_x shape before reshape: {current_x.shape}\")\n",
        "\n",
        "            # Reshape for DCRNN [batch_size * num_nodes, features]\n",
        "            current_x = current_x.reshape(batch_size * num_nodes, features)\n",
        "            #print(f\"Timestep {t} - current_x shape after reshape: {current_x.shape}\")\n",
        "\n",
        "            # First DCRNN layer\n",
        "            #print(f\"Timestep {t} - Before DCRNN1 - input shape: {current_x.shape}, h1 shape: {None if h1 is None else h1.shape}\")\n",
        "            h1 = self.dcrnn1(current_x, edge_index, edge_weight, h1)\n",
        "            #print(f\"Timestep {t} - After DCRNN1 - h1 shape: {h1.shape}\")\n",
        "            h1 = F.relu(h1)\n",
        "\n",
        "            # Second DCRNN layer\n",
        "            #print(f\"Timestep {t} - Before DCRNN2 - input shape: {h1.shape}, h2 shape: {None if h2 is None else h2.shape}\")\n",
        "            h2 = self.dcrnn2(h1, edge_index, edge_weight, h2)\n",
        "            #print(f\"Timestep {t} - After DCRNN2 - h2 shape: {h2.shape}\")\n",
        "            h2 = F.relu(h2)\n",
        "\n",
        "        #print(f\"\\nFinal shapes before linear:\")\n",
        "        #print(f\"h2 shape: {h2.shape}\")\n",
        "        out = self.linear(h2)\n",
        "        #print(f\"Output shape after linear: {out.shape}\")\n",
        "        #print(f\"Final reshape target: [batch_size={batch_size}, num_nodes={num_nodes}, -1]\")\n",
        "\n",
        "        # Return predictions reshaped to [batch_size, num_nodes, output_timesteps]\n",
        "        return out.reshape(batch_size, num_nodes, -1)\n",
        "\n",
        "# Update training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RecurrentGCN(node_features = 2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, eps=1e-3)  # Set epsilon\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                               milestones=[20,30,40,50],\n",
        "                                               gamma=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "model.train()\n",
        "batch_size = 256\n",
        "\n",
        "print(f\"Scaler is enabled: {scaler.is_enabled()}\")\n",
        "for epoch in tqdm(range(100)):\n",
        "    cost = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_snapshots = list(train_dataset)\n",
        "\n",
        "    for i in range(0, len(train_snapshots), batch_size):\n",
        "        batch_snapshots = train_snapshots[i:i+batch_size]\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            # Keep original shape when stacking\n",
        "            batch_x = torch.stack([df.scaler.normalize(snapshot.x) for snapshot in batch_snapshots]).to(device)\n",
        "            batch_edge_index = batch_snapshots[0].edge_index.to(device)\n",
        "            batch_edge_attr = batch_snapshots[0].edge_attr.to(device)\n",
        "            batch_y = torch.stack([snapshot.y[:, 0, :] for snapshot in batch_snapshots]).to(device)\n",
        "            # Normalize targets (y) if they are not already normalized\n",
        "            #batch_y = torch.stack([snapshot.y[:, 0, :] for snapshot in batch_snapshots]).to(device)\n",
        "            #batch_y = torch.stack([df.scaler.normalize(snapshot.y[:, 0, :]) for snapshot in batch_snapshots]).to(device)\n",
        "            # for snapshot in batch_snapshots:\n",
        "            #   print(\"snapshot y shape\", snapshot.y.shape)\n",
        "            #   print(\"snapshot x shape\", snapshot.x.shape)\n",
        "            # Get predictions for all horizons\n",
        "            batch_y_15 = torch.stack([snapshot.y[:, 0, 2] for snapshot in batch_snapshots]).to(device)  # 15min (3rd step)\n",
        "            batch_y_30 = torch.stack([snapshot.y[:, 0, 5] for snapshot in batch_snapshots]).to(device)  # 30min (6th step)\n",
        "            batch_y_60 = torch.stack([snapshot.y[:, 0, 11] for snapshot in batch_snapshots]).to(device) # 60min (12th step)\n",
        "\n",
        "            y_hat = model(batch_x, batch_edge_index, batch_edge_attr)\n",
        "            #print(\"y hat shape\", y_hat.shape)\n",
        "            y_hat_15 = y_hat[:, :, 2]  # 15min prediction\n",
        "            y_hat_30 = y_hat[:, :, 5]  # 30min prediction\n",
        "            y_hat_60 = y_hat[:, :, 11] # 60min prediction\n",
        "            #print(\"\\ny hat 15\", type(y_hat_15), y_hat_15.shape)\n",
        "            #print(\"y hat 30\", type(y_hat_30), y_hat_30.shape)\n",
        "            #print(\"y hat 60\", type(y_hat_60), y_hat_60.shape)\n",
        "            # Denormalize predictions and targets\n",
        "            y_hat = df.scaler.unnormalize_y(y_hat)\n",
        "            y_hat_15 = df.scaler.unnormalize_y(y_hat_15)\n",
        "            y_hat_30 = df.scaler.unnormalize_y(y_hat_30)\n",
        "            y_hat_60 = df.scaler.unnormalize_y(y_hat_60)\n",
        "            #y_true_15 = df.scaler.unnormalize_y(batch_y_15)\n",
        "            #y_true_30 = df.scaler.unnormalize_y(batch_y_30)\n",
        "            #y_true_60 = df.scaler.unnormalize_y(batch_y_60)\n",
        "\n",
        "            #print(\"y hat 15\", y_hat_15)\n",
        "            #print(\"y true 15\", y_true_15)\n",
        "            #print(\"batch true 15\", batch_y_15)\n",
        "            # Calculate MSE for each horizon\n",
        "            rmse_15 = torch.sqrt(torch.mean((y_hat_15 - batch_y_15)**2))\n",
        "            rmse_30 = torch.sqrt(torch.mean((y_hat_30 - batch_y_30)**2))\n",
        "            rmse_60 = torch.sqrt(torch.mean((y_hat_60 - batch_y_60)**2))\n",
        "\n",
        "            null_val = 0.\n",
        "            #print(\"rmse 15 shape\", rmse_15)\n",
        "            #print(\"y hat\", y_hat)\n",
        "            #print(\"batch y\", batch_y)\n",
        "            print(f\"y_hat device: {y_hat.device}\")\n",
        "            print(f\"batch_y device: {batch_y.device}\")\n",
        "            print(f\"y_hat dtype: {y_hat.dtype}\")\n",
        "            print(f\"batch_y dtype: {batch_y.dtype}\")\n",
        "            batch_cost = rmse_15#masked_mae_tf(y_hat.detach().cpu().numpy(), batch_y.detach().cpu().numpy(), null_val)\n",
        "            print(\"Masked mse: \", batch_cost)\n",
        "            #batch_cost = rmse_15\n",
        "            #null_val = 0.\n",
        "            #batch_cost = masked_mae_loss(scaler, null_val)(y_hat.detach().cpu().numpy(), batch_y.detach().cpu().numpy())\n",
        "            #batch_cost = torch.mean((y_hat - batch_y)**2)\n",
        "\n",
        "        del batch_x, batch_edge_index, batch_edge_attr, y_hat, y_hat_15, y_hat_30, y_hat_60, batch_y_15, batch_y_30, batch_y_60\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        scaler.scale(batch_cost).backward()\n",
        "        cost += batch_cost.item()\n",
        "\n",
        "    print(f\"Epoch {epoch} - Gradient norm before unscaling: {torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0).item()}\")\n",
        "    scaler.unscale_(optimizer)\n",
        "    print(f\"Epoch {epoch} - Gradient norm after unscaling: {torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0).item()}\")\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad()\n",
        "    scheduler.step()\n",
        "\n",
        "    num_train_batches = (len(train_snapshots) + batch_size - 1) // batch_size\n",
        "    loss = cost/num_train_batches\n",
        "\n",
        "    print(f'Epoch {epoch}, LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {loss:.4f}, '\n",
        "          f'RMSE@15min: {rmse_15.item():.4f}, RMSE@30min: {rmse_30.item():.4f}, RMSE@60min: {rmse_60.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGo7C0iP7dWc",
        "outputId": "05a4db40-10c7-4ed8-a032-45d22c4ab10b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.9042, device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "cost = 0\n",
        "rmse_15_total = 0\n",
        "rmse_30_total = 0\n",
        "rmse_60_total = 0\n",
        "mae_15_total = 0\n",
        "mae_30_total = 0\n",
        "mae_60_total = 0\n",
        "mape_15_total = 0\n",
        "mape_30_total = 0\n",
        "mape_60_total = 0\n",
        "\n",
        "test_snapshots = list(test_dataset)\n",
        "batch_size = 512  # Can use larger batch size for evaluation since we don't store gradients\n",
        "\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    for i in range(0, len(test_snapshots), batch_size):\n",
        "        batch_snapshots = test_snapshots[i:i+batch_size]\n",
        "\n",
        "        # Process entire batch at once\n",
        "        batch_x = torch.stack([df.scaler.normalize(snapshot.x) for snapshot in batch_snapshots]).to(device)\n",
        "        batch_edge_index = batch_snapshots[0].edge_index.to(device)\n",
        "        batch_edge_attr = batch_snapshots[0].edge_attr.to(device)\n",
        "        batch_y = torch.stack([snapshot.y[:, 0, :] for snapshot in batch_snapshots]).to(device)\n",
        "\n",
        "        # Get predictions for all horizons\n",
        "        y_hat = model(batch_x, batch_edge_index, batch_edge_attr)\n",
        "        y_hat_15 = y_hat[:, :, 2]  # 15min prediction\n",
        "        y_hat_30 = y_hat[:, :, 5]  # 30min prediction\n",
        "        y_hat_60 = y_hat[:, :, 11] # 60min prediction\n",
        "\n",
        "        # Denormalize predictions and targets\n",
        "        y_hat_15 = df.scaler.unnormalize_y(y_hat_15)\n",
        "        y_hat_30 = df.scaler.unnormalize_y(y_hat_30)\n",
        "        y_hat_60 = df.scaler.unnormalize_y(y_hat_60)\n",
        "\n",
        "        # Calculate RMSE, MAE, and MAPE for each horizon\n",
        "        rmse_15 = torch.sqrt(torch.mean((y_hat_15 - batch_y_15)**2))\n",
        "        rmse_30 = torch.sqrt(torch.mean((y_hat_30 - batch_y_30)**2))\n",
        "        rmse_60 = torch.sqrt(torch.mean((y_hat_60 - batch_y_60)**2))\n",
        "\n",
        "        mae_15 = torch.mean(torch.abs(y_hat_15 - batch_y_15))\n",
        "        mae_30 = torch.mean(torch.abs(y_hat_30 - batch_y_30))\n",
        "        mae_60 = torch.mean(torch.abs(y_hat_60 - batch_y_60))\n",
        "\n",
        "        mape_15 = torch.mean(torch.abs((y_hat_15 - batch_y_15) / batch_y_15)) * 100\n",
        "        mape_30 = torch.mean(torch.abs((y_hat_30 - batch_y_30) / batch_y_30)) * 100\n",
        "        mape_60 = torch.mean(torch.abs((y_hat_60 - batch_y_60) / batch_y_60)) * 100\n",
        "\n",
        "        # Accumulate metrics for logging\n",
        "        rmse_15_total += rmse_15.item()\n",
        "        rmse_30_total += rmse_30.item()\n",
        "        rmse_60_total += rmse_60.item()\n",
        "        mae_15_total += mae_15.item()\n",
        "        mae_30_total += mae_30.item()\n",
        "        mae_60_total += mae_60.item()\n",
        "        mape_15_total += mape_15.item()\n",
        "        mape_30_total += mape_30.item()\n",
        "        mape_60_total += mape_60.item()\n",
        "\n",
        "        # Calculate batch cost (could use MSE as a proxy here)\n",
        "        batch_cost = torch.mean((y_hat - batch_y)**2)\n",
        "        cost += batch_cost.item()\n",
        "\n",
        "        del batch_x, batch_edge_index, batch_edge_attr, y_hat, batch_y\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Calculate average metrics\n",
        "num_batches = (len(test_snapshots) + batch_size - 1) // batch_size\n",
        "cost /= num_batches\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Evaluation Loss: {cost:.4f}\")\n",
        "print(f\"RMSE@15min: {rmse_15_total / num_batches:.4f}\")\n",
        "print(f\"RMSE@30min: {rmse_30_total / num_batches:.4f}\")\n",
        "print(f\"RMSE@60min: {rmse_60_total / num_batches:.4f}\")\n",
        "print(f\"MAE@15min: {mae_15_total / num_batches:.4f}\")\n",
        "print(f\"MAE@30min: {mae_30_total / num_batches:.4f}\")\n",
        "print(f\"MAE@60min: {mae_60_total / num_batches:.4f}\")\n",
        "print(f\"MAPE@15min: {mape_15_total / num_batches:.4f}%\")\n",
        "print(f\"MAPE@30min: {mape_30_total / num_batches:.4f}%\")\n",
        "print(f\"MAPE@60min: {mape_60_total / num_batches:.4f}%\")"
      ],
      "metadata": {
        "id": "eMZInTTzrO7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBRcgEGjV3Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "model.eval()\n",
        "cost = 0\n",
        "test_snapshots = list(test_dataset)\n",
        "batch_size = 512  # Can use larger batch size for evaluation since we don't store gradients\n",
        "\n",
        "predictions = []\n",
        "truths = []\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    for i in range(0, len(test_snapshots), batch_size):\n",
        "        batch_snapshots = test_snapshots[i:i+batch_size]\n",
        "\n",
        "        # Process entire batch at once\n",
        "        batch_x = torch.stack([df.scaler.transform(snapshot.x) for snapshot in batch_snapshots]).to(device)\n",
        "        batch_edge_index = batch_snapshots[0].edge_index.to(device)\n",
        "        batch_edge_attr = batch_snapshots[0].edge_attr.to(device)\n",
        "        batch_y = torch.stack([snapshot.y[:, 0, :] for snapshot in batch_snapshots]).to(device)\n",
        "\n",
        "        # Forward pass on entire batch\n",
        "        y_hat = model(batch_x, batch_edge_index, batch_edge_attr)\n",
        "\n",
        "        # Denormalize predictions and ground truth\n",
        "        y_hat_denorm = df.scaler.inverse_transform(y_hat)\n",
        "        y_true_denorm = df.scaler.inverse_transform(batch_y)\n",
        "\n",
        "        # Calculate MSE on denormalized values\n",
        "        batch_cost = torch.mean((y_hat_denorm - y_true_denorm)**2)\n",
        "        cost += batch_cost.item()\n",
        "\n",
        "        del batch_x, batch_edge_index, batch_edge_attr, y_hat, batch_y\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "num_batches = (len(test_snapshots) + batch_size - 1) // batch_size\n",
        "mse = cost / num_batches\n",
        "rmse = math.sqrt(mse)\n",
        "print(\"MSE: {:.4f}\".format(mse))\n",
        "print(\"RMSE: {:.4f}\".format(rmse))"
      ],
      "metadata": {
        "id": "Q726m6X7TA91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1LvsJSowRUm",
        "outputId": "bbcfeb3d-ce54-4396-b4c1-0ace7111bc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36457"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_hat.shape,snapshot.y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwS3r801K5S9",
        "outputId": "d30960d5-6744-4906-c11e-f546b557bb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([325, 12]) torch.Size([325, 2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3zC5jERyS7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7058232b-bf5c-440b-9ac5-607c6f31407e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([207, 2, 12]) torch.Size([2, 1722]) torch.Size([1722])\n"
          ]
        }
      ],
      "source": [
        "print(snapshot.x.shape, snapshot.edge_index.shape, snapshot.edge_attr.shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}